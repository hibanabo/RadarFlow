"""æ¼”ç¤ºå¦‚ä½•é€šè¿‡ç»Ÿä¸€æ¥å£æŠ“å–å¤šæ¥æºæ–°é—»ï¼Œå¹¶è¾“å‡º JSONã€‚"""
from __future__ import annotations

import logging
from pathlib import Path
from typing import Dict, Iterable, List

from ai import AIClient, AISummary, AISummaryFilter, AIPreFilter
from deduper import SQLiteDeduper
from fetcher import collect_news
from filters import FilterSet
from notifications import NotificationClient
from utils.storage import SQLiteStorage
from utils.time_utils import get_timezone_helper

logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s", force=True)


def log_section(title: str) -> None:
    logging.info("%s %s %s", "=" * 12, title, "=" * 12)


def main() -> None:
    news = list(collect_news())
    logging.info("å…±æ‹‰å– %d æ¡æ–°é—»", len(news))
    tz_helper = get_timezone_helper()
    for item in news:
        if item.published_at:
            converted = tz_helper.to_iso(item.published_at)
            if converted:
                if isinstance(item.raw, dict):
                    item.raw.setdefault("original_published_at", item.published_at)
                    item.raw["published_at"] = converted
                item.published_at = converted
    for item in news:
        timestamp_raw = item.published_at or (item.raw.get("timestamp") if isinstance(item.raw, dict) else None)
        timestamp = tz_helper.to_display(timestamp_raw) or timestamp_raw or "æœªçŸ¥æ—¶é—´"
        authors = ", ".join(item.authors) if getattr(item, "authors", None) else None
        logging.info(
            "%s - %s | ğŸ“… %s | âœï¸ %s",
            item.source,
            item.title,
            timestamp,
            authors or "æœªçŸ¥ä½œè€…",
        )

    db_path = Path("state") / "news.db"
    deduper = SQLiteDeduper(db_path, retention_days=3)
    filter_set = FilterSet()
    ai_prefilter = AIPreFilter()
    ai_filter = AISummaryFilter()
    try:
        log_section("å»é‡")
        fresh_news = deduper.filter_new(news)
        logging.info("å»é‡åæ–°å¢ %d/%d æ¡æ–°é—»", len(fresh_news), len(news))

        has_active_rules = any(rule.enabled for rule in filter_set.rules)
        log_section("AI é¢„è¿‡æ»¤")
        prefilter_active = (
            ai_prefilter.enabled
            and bool(ai_prefilter.api_key)
            and filter_set.enabled
            and has_active_rules
        )
        if prefilter_active:
            logging.info("AI é¢„è¿‡æ»¤è¾“å…¥ %d æ¡æ–°é—»", len(fresh_news))
            prefiltered_news = ai_prefilter.apply(fresh_news, filter_set.rules, filter_set.enabled)
            logging.info("AI é¢„è¿‡æ»¤è¾“å‡º %d æ¡æ–°é—»", len(prefiltered_news))
        else:
            logging.info("AI é¢„è¿‡æ»¤æœªå¯ç”¨æˆ–ç¼ºå°‘å¿…è¦é…ç½®ï¼Œè·³è¿‡ã€‚")
            prefiltered_news = list(fresh_news)

        log_section("å…³é”®è¯è¿‡æ»¤")
        logging.info("å…³é”®è¯è¿‡æ»¤è¾“å…¥ %d æ¡æ–°é—»", len(prefiltered_news))
        filtered_news = filter_set.apply(prefiltered_news)

        ai_client = AIClient()
        summaries: List[AISummary] = []
        log_section("AI æ‘˜è¦")
        if ai_client.enabled and filtered_news:
            max_items = getattr(ai_client, "max_items", len(filtered_news)) or len(filtered_news)
            if max_items <= 0:
                target_count = len(filtered_news)
            else:
                target_count = min(max_items, len(filtered_news))
            ai_targets = filtered_news[:target_count]
            logging.info("AI å°†å¤„ç† %d æ¡æ–°é—»", len(ai_targets))
            summaries = ai_client.summarize_news(ai_targets)
        else:
            logging.info("AI æ‘˜è¦æœªå¯ç”¨æˆ–æ— å¯å¤„ç†æ–°é—»ï¼Œè·³è¿‡ã€‚")

        blocked_by_ai = [
            record
            for record in filtered_news
            if isinstance(record.raw, dict) and record.raw.get("_ai_summary_blocked")
        ]
        if blocked_by_ai:
            filtered_news = [
                record
                for record in filtered_news
                if not (isinstance(record.raw, dict) and record.raw.get("_ai_summary_blocked"))
            ]
            logging.warning("AI æ‘˜è¦é˜¶æ®µæ‹¦æˆª %d æ¡æ–°é—»ï¼Œå·²è·³è¿‡åç»­æµç¨‹ã€‚", len(blocked_by_ai))

        summary_map = {
            (summary.url or f"{summary.source}-{summary.title}"): summary
            for summary in (summaries or [])
        }
        log_section("AI åç½®è¿‡æ»¤")
        logging.info("AI åç½®è¿‡æ»¤è¾“å…¥ %d æ¡æ–°é—»", len(filtered_news))
        post_filtered_news, post_filtered_summary_map = ai_filter.apply(filtered_news, summary_map)
        logging.info("AI åç½®è¿‡æ»¤è¾“å‡º %d æ¡æ–°é—»", len(post_filtered_news))

        storage = SQLiteStorage(db_path)
        try:
            storage.save_news(filtered_news, summary_map)
        finally:
            storage.close()

        notifier = NotificationClient()
        log_section("é€šçŸ¥æ¨é€")
        logging.info("å°†æ¨é€ %d æ¡æ–°é—»", len(post_filtered_news))
        results = notifier.send(post_filtered_news, post_filtered_summary_map)
        if results:
            logging.info("é€šçŸ¥å‘é€ç»“æœ: %s", results)

        for item in fresh_news:
            deduper.mark(item)
    finally:
        deduper.close()


if __name__ == "__main__":
    main()
